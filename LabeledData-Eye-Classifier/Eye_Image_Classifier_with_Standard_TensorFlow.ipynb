{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grewe/StrokeChangeMLTraining/blob/main/LabeledData-Eye-Classifier/Eye_Image_Classifier_with_Standard_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScitaPqhKtuW"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ImageClassifier from Standard Tensorflow hub model -retrain, eval and test**\n",
        "\n",
        "\n",
        "### In this colab notebook, you'll learn how to\n",
        " \n",
        "\n",
        "*  train a custom image classifer from a pre-existing model \n",
        "*  tune parameters\n",
        "*  evaluate it\n",
        "*  run test cases and plot results\n",
        "*  run tensorboard\n",
        "*  save it to Saved_Model\n",
        "*  export as TFLite\n",
        "\n",
        "\n",
        "This colab is expecting the data to be already split into separate train, test and valid folders each containing subfolders - one per class. For example, in the classification of eyes for stroke patients we have the classes of normalEye, strokeEyeWeak, strokeEyeMid, strokeEyeSevere.  Hence the train and test and valid folders will contain the following sub-folders\n",
        "\n",
        "*  test/normalEye\n",
        "*  test/strokeEyeWeak\n",
        "*  test/strokeEyeMid\n",
        "*  test/strokeEyeSevere\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **IMPORTANT:  TWO OPTIONS for training 1= from scratch and 2=from a checkpoint (i.e. if colab stopped and you had to restart training).    This will effect the following cells of evaluation and exporting.  NOTE: this is a work around because the Model Maker API does not yet support loading from checkpoints to train.   UNFORTUANTELY, this means that what happens is the underlying Object Detection API is used which requires a slightly different data input and all of this is prepared for you.**  \n",
        "\n",
        "*   #### **BOTH options export and work in the default Model Maker Android app as shown in below in a screenshot (I tested it)**\n"
      ],
      "metadata": {
        "id": "6w9q_Hvv-sJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "Vr1T2-8v_v4E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvztxQ6VsK2k"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 0: mount Drive and install libraries and import packages\n",
        "\n",
        "### assumes data in drive/Mask/MaskDetect/data\n",
        "\n",
        "\n",
        "Here you'll use the dataset given in the zip file (http://borg.csueastbay.edu/~grewe/CS663/Mat/TensorFlow/TensorFlowLiteModelMaker/data.zip).\n",
        "\n",
        "*  grab the zip and unzip and upload the content to your drive in the path YOUDRIVE/Mask/MaskDetect"
      ],
      "metadata": {
        "id": "kBrg4coG_6UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "fjGm6fLx_9AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "## STEP 1:  SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlauq-4FWGZM"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "## Select the TF2 SavedModel module to use & specify BATCH_SIZE, determine IMAGE_SIZE to allign with model input size\n",
        "\n",
        "For starters, use [https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4). The same URL can be used in code to identify the SavedModel and in your browser to show its documentation. (Note that models in TF1 Hub format won't work here.)\n",
        "\n",
        "You can find more TF2 models that generate image feature vectors [here](https://tfhub.dev/s?module-type=image-feature-vector&tf-version=tf2).\n",
        "\n",
        "There are multiple possible models to try. All you need to do is select a different one on the cell below and follow up with the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlsEcKVeuCnf"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "model_name = \"efficientnetv2-b0\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"pnasnet_large\": 331,\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map.get(model_name)\n",
        "pixels = model_image_size_map.get(model_name, 224)\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(f\"Input size {IMAGE_SIZE}\")\n",
        "\n",
        "BATCH_SIZE = 16 #@param {type:\"integer\"}\n",
        "print(f\"Batch size {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTY8qzyYv3vl"
      },
      "source": [
        "# STEP 2:  Set up the  dataset\n",
        "\n",
        "Again assumes that the data is already augmented as desired and stored in google drive under train, test, valid  with sub-directories of class names."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup the directory names\n",
        "\n",
        "image_dir=\"drive/MyDrive/LabeledData-Eye-Classifier-4Classes\"\n",
        "print(\"image_dir\\n\")\n",
        "print(image_dir)\n",
        "\n",
        "test_dir=\"drive/MyDrive/LabeledData-Eye-Classifier-4Classes/test\"\n",
        "train_dir=\"drive/MyDrive/LabeledData-Eye-Classifier-4Classes/train\"\n",
        "valid_dir=\"drive/MyDrive/LabeledData-Eye-Classifier-4Classes/valid\"\n",
        "\n",
        "\n",
        "print(train_dir)\n",
        "print(valid_dir)\n",
        "print(test_dir)\n",
        "\n",
        "%ls -1 {train_dir} | wc -l\n",
        "%ls -1 {valid_dir} | wc -l\n",
        "%ls -1 {test_dir} | wc -l"
      ],
      "metadata": {
        "id": "BWVw8sgWAwV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROBLEM --what is repeat in the next code block?\n",
        "\n",
        "Repeat is specified for infinite datasets. In such case steps per epoch needs to be defined in model.fit()"
      ],
      "metadata": {
        "id": "8JA_MtTfW-Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see for documentaton on dataset creation https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory\n",
        "# will resize the images as appropriate to IMAGE_SIZExIMAGE_SIZE as defined above by model selection\n",
        "\n",
        "\n",
        "#SETUP TRAINING DATA  tf.data.Dataset\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      train_dir,\n",
        "      label_mode=\"categorical\",\n",
        "      image_size=IMAGE_SIZE,\n",
        "      batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "#determine classnames\n",
        "class_names = tuple(train_ds.class_names)\n",
        "\n",
        "#batch the training data by BATCH_SIZE specified above\n",
        "train_size = train_ds.cardinality().numpy()\n",
        "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
        "\n",
        "# setup images so will be rescaled as input from 0to255 to 0.0 to 1.0\n",
        "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
        "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
        "train_ds = train_ds.map(lambda images, labels:\n",
        "                        (preprocessing_model(images), labels))\n",
        "\n",
        "#SETUP TEST Dataset\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      test_dir,\n",
        "      label_mode=\"categorical\",\n",
        "      image_size=IMAGE_SIZE,\n",
        "      batch_size=BATCH_SIZE)\n",
        "#batch the training data by BATCH_SIZE specified above\n",
        "test_size = test_ds.cardinality().numpy()\n",
        "test_ds = test_ds.unbatch().batch(BATCH_SIZE)\n",
        "test_ds = test_ds.map(lambda images, labels:\n",
        "                        (preprocessing_model(images), labels))\n",
        "\n",
        "\n",
        "#SETUP VALIDATION Dataset\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      test_dir,\n",
        "      label_mode=\"categorical\",\n",
        "      image_size=IMAGE_SIZE,\n",
        "      batch_size=BATCH_SIZE)\n",
        "valid_size = val_ds.cardinality().numpy()\n",
        "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(lambda images, labels:\n",
        "                    (normalization_layer(images), labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "TrVQuw9QAZyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_gVStowW3G"
      },
      "source": [
        "# **STEP 3: Defining the model**\n",
        "\n",
        "All it takes is to put a linear classifier on top of the `feature_extractor_layer` with the Hub module.\n",
        "\n",
        "For speed, we start out with a non-trainable `feature_extractor_layer`, but you can also enable fine-tuning for greater accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaJW3XrPyFiF"
      },
      "outputs": [],
      "source": [
        "do_fine_tuning = True #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50FYNIb1dmJH"
      },
      "outputs": [],
      "source": [
        "print(\"Building model with\", model_handle)\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(len(class_names),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "#  **STEP 4: Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f3yBUvkd_VJ"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.000001, momentum=0.9), # try one parameter tuning each training momentum=0.9, weight_decay=0.001\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "# see https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint \n",
        "checkpoints_dir = \"drive/MyDrive/LabeledData-Eye-Classifier-4Classes/checkpoints-learningrate-000001/checkpoint-{epoch:02d}.hdf5\"\n",
        "\n",
        "mc = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoints_dir, save_best_only=True, verbose=1, save_freq='epoch')   # save_weights_only=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_YKX2Qnfg6x"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = valid_size // BATCH_SIZE\n",
        "epochs = 2500\n",
        "\n",
        "# hist = model.fit(\n",
        "#     train_ds,\n",
        "#     epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
        "#     validation_data=val_ds,\n",
        "#     validation_steps=validation_steps,\n",
        "#     callbacks=[mc]).history\n",
        "\n",
        "log_dir = \"Logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, update_freq='epoch')\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs, \n",
        "    batch_size = BATCH_SIZE,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[tensorboard_callback,mc])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the pid of the process using port 6006\n",
        "!lsof -i:6006\n"
      ],
      "metadata": {
        "id": "ECVUhEvK0pEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 3060"
      ],
      "metadata": {
        "id": "s43nZ5qu-lqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir drive/MyDrive/Logs/"
      ],
      "metadata": {
        "id": "rSdx0dzkbqV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYOw0fTO1W4x",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"], color='orange')\n",
        "plt.plot(hist[\"val_loss\"], color='blue') \n",
        "plt.legend([\"train\", \"val\"], loc =\"lower right\")\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"], color='orange')\n",
        "plt.plot(hist[\"val_accuracy\"], color='blue')\n",
        "\n",
        "plt.legend([\"blue\", \"green\"], loc =\"lower right\")\n",
        "plt.legend([\"train\", \"val\"], loc =\"lower right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load from saved weights**"
      ],
      "metadata": {
        "id": "1qiDD-4z2p35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building model with\", model_handle)\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(len(class_names),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nFJUAtC42pbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.000001), # try one parameter tuning each training momentum=0.9, weight_decay=0.001\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Aes7y-Er208R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set number of epoch and batch_size to train and then train\n",
        "epochs = 1000\n",
        "batch_size = BATCH_SIZE"
      ],
      "metadata": {
        "id": "e4cvmrl1ZXf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, acc = model.evaluate(test_ds, verbose=2)\n",
        "print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))"
      ],
      "metadata": {
        "id": "c0o79Jl13IGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path of model checkpoint\n",
        "checkpoint_path = \"drive/MyDrive/LabeledData-Eye-Classifier-4Classes/checkpoints-learningrate-000001/run2/checkpoint-971.hdf5\"\n",
        "\n",
        "# Loads the weights\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "# Re-evaluate the model\n",
        "loss, acc = model.evaluate(test_ds, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
      ],
      "metadata": {
        "id": "YDKIZm9X3MSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "log_dir = \"Logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, update_freq='epoch')\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs, \n",
        "    batch_size = BATCH_SIZE,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[tensorboard_callback,mc])"
      ],
      "metadata": {
        "id": "k00bpr5obgnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ8DKKgeKv4-"
      },
      "source": [
        "# **STEP 5:  test** -Try out the model on an image from the **test** data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi1iCNB9K1Ai"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(test_ds))\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0]) #this seems wrong\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "print(\"True label: \" + class_names[true_index])\n",
        "print(\"Predicted label: \" + class_names[predicted_index])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION --did I fix the iterator to only get unique samples from test_ds**"
      ],
      "metadata": {
        "id": "5fzQ6Lb5WqRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function that returns 'red'/'black' depending on if its two input\n",
        "# parameter matches or not.\n",
        "def get_label_color(val1, val2):\n",
        "  if val1 == val2:\n",
        "    return 'black'\n",
        "  else:\n",
        "    return 'red'\n",
        "\n",
        "# Then plot  100 test images and their predicted labels.\n",
        "# If a prediction result is different from the label provided label in \"test\"\n",
        "# dataset, we will highlight it in red color.\n",
        "\n",
        "plt.figure(figsize=(30, 30))\n",
        "pred=[]\n",
        "truth=[]\n",
        "predicts = model.predict(test_ds)\n",
        "#predicts = model.predict_top_k(test_ds)\n",
        "# takes 100 images at a time\n",
        "for i, (image, label) in enumerate(test_ds.unbatch().take(64)):\n",
        "  ax = plt.subplot(8, 8, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
        "  predicted_index = np.argmax(predicts[i])\n",
        "  predict_label = class_names[predicted_index]\n",
        "  \n",
        "  true_index = np.argmax(label)\n",
        "  truth_label = class_names[true_index]\n",
        "\n",
        "  color = get_label_color(predict_label, truth_label)\n",
        "  ax.xaxis.label.set_color(color)\n",
        "  #plt.xlabel('Predicted: %s \\n Truth %s',  predict_label, test_data.index_to_label[label.numpy()])\n",
        "  plt.xlabel('Pred: %s \\n Truth %s' % (predict_label, truth_label))\n",
        "  truth.append(truth_label)\n",
        "  pred.append(predict_label)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DCgbMf5iMV2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement confusion matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "print(truth)\n",
        "print(pred)\n",
        "\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(truth, pred, normalize=None)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"normalEye\",\"strokeEyeMid\",\"strokeEyeSevere\",\"strokeEyeWeak\"])\n",
        "\n",
        "\n",
        "cm_display.plot()\n",
        "plt.xticks(rotation=90)\n",
        "plt.figure(figsize=(100, 100))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6UzZMQ5CxyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCsAsQM1IRvA"
      },
      "source": [
        "# **STEP 6: Save model as TFlite**\n",
        "\n",
        "Finally, the trained model can be saved for deployment to TF Serving or TFLite (on mobile) as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGvTi69oIc2d"
      },
      "outputs": [],
      "source": [
        "saved_model_path = f\"drive/MyDrive/LabeledData-Eye-Classifier-4Classes/checkpoints-learningrate-000001/models-new/{model_name}\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert saved model to tflite\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path) # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('drive/MyDrive/LabeledData-Eye-Classifier-4Classes/checkpoints-learningrate-000001/models/model-3500.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "JNv59ePk-oV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzW4oNRjILaq"
      },
      "source": [
        "## NOT TESTED YET: Deployment to TensorFlow Lite\n",
        "\n",
        "[TensorFlow Lite](https://www.tensorflow.org/lite) lets you deploy TensorFlow models to mobile and IoT devices. The code below shows how to convert the trained model to TFLite and apply post-training tools from the [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization). Finally, it runs it in the TFLite Interpreter to examine the resulting quality\n",
        "\n",
        "  * Converting without optimization provides the same results as before (up to roundoff error).\n",
        "  * Converting with optimization without any data quantizes the model weights to 8 bits, but inference still uses floating-point computation for the neural network activations. This reduces model size almost by a factor of 4 and improves CPU latency on mobile devices.\n",
        "  * On top, computation of the neural network activations can be quantized to 8-bit integers as well if a small reference dataset is provided to calibrate the quantization range. On a mobile device, this accelerates inference further and makes it possible to run on accelerators like Edge TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va1Vo92fSyV6"
      },
      "outputs": [],
      "source": [
        "#@title Optimization settings\n",
        "optimize_lite_model = False  #@param {type:\"boolean\"}\n",
        "#@markdown Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
        "num_calibration_examples = 60  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "representative_dataset = None\n",
        "if optimize_lite_model and num_calibration_examples:\n",
        "  # Use a bounded number of training examples without labels for calibration.\n",
        "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
        "  representative_dataset = lambda: itertools.islice(\n",
        "      ([image[None, ...]] for batch, _ in train_ds for image in batch),\n",
        "      num_calibration_examples)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "if optimize_lite_model:\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  if representative_dataset:  # This is optional, see above.\n",
        "    converter.representative_dataset = representative_dataset\n",
        "lite_model_content = converter.convert()\n",
        "\n",
        "with open(f\"/tmp/lite_flowers_model_{model_name}.tflite\", \"wb\") as f:\n",
        "  f.write(lite_model_content)\n",
        "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wqEmD0xIqeG"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
        "# This little helper wraps the TFLite Interpreter as a numpy-to-numpy function.\n",
        "def lite_model(images):\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "  interpreter.invoke()\n",
        "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMMK-fZrKrk8"
      },
      "outputs": [],
      "source": [
        "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
        "num_eval_examples = 50  #@param {type:\"slider\", min:0, max:700}\n",
        "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
        "                for batch in train_ds\n",
        "                for (image, label) in zip(*batch))\n",
        "count = 0\n",
        "count_lite_tf_agree = 0\n",
        "count_lite_correct = 0\n",
        "for image, label in eval_dataset:\n",
        "  probs_lite = lite_model(image[None, ...])[0]\n",
        "  probs_tf = model(image[None, ...]).numpy()[0]\n",
        "  y_lite = np.argmax(probs_lite)\n",
        "  y_tf = np.argmax(probs_tf)\n",
        "  y_true = np.argmax(label)\n",
        "  count +=1\n",
        "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
        "  if y_lite == y_true: count_lite_correct += 1\n",
        "  if count >= num_eval_examples: break\n",
        "print(\"TFLite model agrees with original model on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
        "print(\"TFLite model is accurate on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}